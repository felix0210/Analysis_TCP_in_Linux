\section{tcp\_shutdown}
			\label{ClientSendFin:tcp_shutdown}
			通过shutdown系统调用，主动关闭TCP连接。该系统调用最终由\mintinline{c}{tcp_shutdown}实现。
\begin{minted}[linenos]{c}
/*
Location:

	net/ipv4/tcp.c

Function:

	Shutdown the sending side of a connection. Much like close except
	that we don't receive shut down or sock_set_flag(sk, SOCK_DEAD).

Parameter:

	sk:传输控制块
	how:???
*/
void tcp_shutdown(struct sock *sk, int how)
{
        /*      We need to grab some memory, and put together a FIN,
         *      and then put it into the queue to be sent.
         *              Tim MacKenzie(tym@dibbler.cs.monash.edu.au) 4 Dec '92.
         */
        if (!(how & SEND_SHUTDOWN))
                return;

        /*
			发送方向的关闭，并且TCP状态为ESTABLISHED、SYN_SENT、
			SYN_RECV、CLOSE_WAIT状态中的一个。
		*/
        if ((1 << sk->sk_state) &
            (TCPF_ESTABLISHED | TCPF_SYN_SENT |
             TCPF_SYN_RECV | TCPF_CLOSE_WAIT)) {
                /* Clear out any half completed packets.  FIN if needed. */
				/*如果此时已经发送一个FIN了，就跳过。*/                
				if (tcp_close_state(sk))
                        tcp_send_fin(sk);
        }
}
\end{minted}
	
	该函数会在需要发送FIN时，调用\mintinline{c}{tcp_close_state()}来设置TCP的状态。

\section{主动关闭}
\label{Actively Close}

	\subsection{第一次握手:发送FIN}
		\subsubsection{基本调用关系}
		
		\subsubsection{\mintinline{c}{tcp_close}}
			当一段完成数据发送任务之后，应用层即可调用close发送一个FIN来
			终止该方向上的连接，当另一端收到这个FIN后，必须通知应用层，
			另一端已经终止了数据传送。

			而close系统调用在传输层接收的实现就是tcp\_close,这里我们依次分析。
\begin{minted}[linenos]{c}
/*
Location:

    net/ipv4/tcp.c

Function:

     进行主动关闭的第一步，发送FIN   

Parameter:

    sk:传输控制块。
    timeout:在真正关闭控制块之前，可以发送剩余数据的时间。
    
*/
void tcp_close(struct sock *sk, long timeout)
{
    struct sk_buff *skb;
    int data_was_unread = 0;
    int state;

    lock_sock(sk);
    sk->sk_shutdown = SHUTDOWN_MASK;

    if (sk->sk_state == TCP_LISTEN) {
        tcp_set_state(sk, TCP_CLOSE);

        /* Special case. */
        inet_csk_listen_stop(sk);

        goto adjudge_to_death;
    }
\end{minted}

        首先，对传输控制块加锁。然后设置关闭标志为SHUTDOWN\_MASK,表示进行双向的关闭。

        如果套接口处于侦听状态，这种情况处理相对比较简单，因为没有建立起连接，因此无需发送FIN等操作。
		设置TCP的状态为CLOSE，然后终止侦听。最后跳转到adjudge\_to\_death处进行相关处理。

\begin{minted}[linenos]{c}
    /*  We need to flush the recv. buffs.  We do this only on the
     *  descriptor close(什么意思), not protocol-sourced(什么意思) closes, because the
     *  reader process may not have drained(消耗) the data yet!
     */
    while ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {
        u32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq;

        if (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)
            len--;
        data_was_unread += len;
        __kfree_skb(skb);
    }

    sk_mem_reclaim(sk);
\end{minted}

        因为要关闭连接，故需要释放已接收队列上的段，同时统计释放了多少数据，然后回收缓存。

\begin{minted}[linenos]{c}
    /* If socket has been already reset (e.g. in tcp_reset()) - kill it. */
    if (sk->sk_state == TCP_CLOSE)
        goto adjudge_to_death;
\end{minted}

        如果socket本身就是close状态的话，直接跳到adjudge\_to\_death就好。

\begin{minted}[linenos]{c}
    /* As outlined in RFC 2525, section 2.17, we send a RST here because
     * data was lost. To witness the awful effects of the old behavior of
     * always doing a FIN, run an older 2.1.x kernel or 2.0.x, start a bulk
     * GET in an FTP client, suspend the process, wait for the client to
     * advertise a zero window, then kill -9 the FTP client, wheee...
     * Note: timeout is always zero in such a case.
     */
	/*
		开启repair选项
	*/
    if (unlikely(tcp_sk(sk)->repair)) {
        sk->sk_prot->disconnect(sk, 0);
    } else if (data_was_unread) {
        /* 
			Unread data was tossed, zap the connection. 
			在存在数据未读的情况下断开连接。这时应该直接置状态
			CLOSE，并主动向对方发送RST，因为这是不正常的情况，
			而发送FIN表示一切正常。
		*/
        NET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);
        tcp_set_state(sk, TCP_CLOSE);
        tcp_send_active_reset(sk, sk->sk_allocation);
    } else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {
        /* Check zero linger _after_ checking for unread data. */
		/*
			如果设置了SO_LINGER并且，延时时间为0,则直接调用disconnect断开
			、删除并释放已建立连接但未被accept的传输控制块。同时删除并释放
			已收到在接收队列、失序队列上的段以及发送队列上的段。
		*/
        sk->sk_prot->disconnect(sk, 0);
        NET_INC_STATS_USER(sock_net(sk), LINUX_MIB_TCPABORTONDATA);
    } else if (tcp_close_state(sk)) {	//判断是否可以发送FIN
        /* We FIN if the application ate all the data before
         * zapping the connection.
         */

        /* RED-PEN. Formally speaking, we have broken TCP state
         * machine. State transitions:
         *
         * TCP_ESTABLISHED -> TCP_FIN_WAIT1
         * TCP_SYN_RECV -> TCP_FIN_WAIT1 (forget it, it's impossible)
         * TCP_CLOSE_WAIT -> TCP_LAST_ACK
         *
         * are legal only when FIN has been sent (i.e. in window),
         * rather than queued out of window. Purists blame.
         *
         * F.e. "RFC state" is ESTABLISHED,
         * if Linux state is FIN-WAIT-1, but FIN is still not sent.
         *
         * The visible declinations are that sometimes
         * we enter time-wait state, when it is not required really
         * (harmless), do not send active resets, when they are
         * required by specs (TCP_ESTABLISHED, TCP_CLOSE_WAIT, when
         * they look as CLOSING or LAST_ACK for Linux)
         * Probably, I missed some more holelets.
         *                      --ANK
         * XXX (TFO) - To start off we don't support SYN+ACK+FIN
         * in a single packet! (May consider it later but will
         * probably need API support or TCP_CORK SYN-ACK until
         * data is written and socket is closed.)
         */
        tcp_send_fin(sk);
    }
    sk_stream_wait_close(sk, timeout);
\end{minted}

	在给对端发送RST或FIN段后，等待套接口的关闭，直到TCP的状态为FIN\_WAIT\_1、CLOSING、LAST\_ACK或等待超时。
\begin{minted}[linenos]{c}
adjudge_to_death:
	/*
		置套接口为DEAD状态，成为孤儿套接口，同时更新系统中的孤儿套接口数。
	*/
    state = sk->sk_state;
    sock_hold(sk);
    sock_orphan(sk);

    /* It is the last release_sock in its life. It will remove backlog. */
    release_sock(sk);

    /* Now socket is owned by kernel and we acquire BH lock
       to finish close. No need to check for user refs.
     */
    local_bh_disable();
    bh_lock_sock(sk);
    WARN_ON(sock_owned_by_user(sk));

    percpu_counter_inc(sk->sk_prot->orphan_count);

    /* Have we already been destroyed by a softirq or backlog? */
    if (state != TCP_CLOSE && sk->sk_state == TCP_CLOSE)
        goto out;
\end{minted}

	接下来处理FIN\_WAIT2到CLOSE状态的转换。
\begin{minted}[linenos]{c}
    /*  This is a (useful) BSD violating of the RFC. There is a
     *  problem with TCP as specified in that the other end could
     *  keep a socket open forever with no application left this end.
     *  We use a 1 minute timeout (about the same as BSD) then kill
     *  our end. If they send after that then tough - BUT: long enough
     *  that we won't make the old 4*rto = almost no time - whoops
     *  reset mistake.
     *
     *  Nope, it was not mistake. It is really desired behaviour
     *  f.e. on http servers, when such sockets are useless, but
     *  consume significant resources. Let's do it with special
     *  linger2 option.                 --ANK
     */

    if (sk->sk_state == TCP_FIN_WAIT2) {
        struct tcp_sock *tp = tcp_sk(sk);
		/*
			如果linger2小于0，则表示无需从TCP_FIN_WAIT2hangtag等待转移到
			CLOSE状态，而是立即设置CLOSE状态，然后给对端发送RST段。
		*/
        if (tp->linger2 < 0) {
            tcp_set_state(sk, TCP_CLOSE);
            tcp_send_active_reset(sk, GFP_ATOMIC);
            NET_INC_STATS_BH(sock_net(sk),
                    LINUX_MIB_TCPABORTONLINGER);
        } else {
			/*计算需要保持TCP_FIN_WAIT2状态的时长*/
            const int tmo = tcp_fin_time(sk);
			/*小于1min，调用TCP_FIN_WAIT2定时器*/
            if (tmo > TCP_TIMEWAIT_LEN) {
                inet_csk_reset_keepalive_timer(sk,
                        tmo - TCP_TIMEWAIT_LEN);
            } else {
			/*
				否则调用timewait控制块取代tcp_sock传出控制块。
			*/
                tcp_time_wait(sk, TCP_FIN_WAIT2, tmo);
                goto out;
            }
        }
    }
\end{minted}

	继续处理。
\begin{minted}[linenos]{c}
	/*不是CLOSE状态*/    
	if (sk->sk_state != TCP_CLOSE) {
        sk_mem_reclaim(sk);
        if (tcp_check_oom(sk, 0)) {
            tcp_set_state(sk, TCP_CLOSE);
            tcp_send_active_reset(sk, GFP_ATOMIC);
            NET_INC_STATS_BH(sock_net(sk),
                    LINUX_MIB_TCPABORTONMEMORY);
        }
    }
	/*是CLOSE状态了*/
    if (sk->sk_state == TCP_CLOSE) {
        struct request_sock *req = tcp_sk(sk)->fastopen_rsk;
        /* We could get here with a non-NULL req if the socket is
         * aborted (e.g., closed with unread data) before 3WHS
         * finishes.
         */
        if (req)
            reqsk_fastopen_remove(sk, req, false);
        inet_csk_destroy_sock(sk);
    }
    /* Otherwise, socket is reprieved until protocol close. */

out:
    bh_unlock_sock(sk);
    local_bh_enable();
    sock_put(sk);
}
\end{minted}

	该函数会根据当前的状态，按照\ref{subsubsec:tcp_state_diagram}中给出的状态图。


		\subsubsection{\mintinline{c}{tcp_close_state}}
\begin{minted}[linenos]{c}
/*
Location:

	net/ipv4/tcp.c

Function:

	进行状态转移，并且判断是否可以发送FIN。

Parameter:

	sk:传输控制块
*/
static int tcp_close_state(struct sock *sk)
{
        int next = (int)new_state[sk->sk_state];
		/*去除可能的FIN_ACTION*/        
		int ns = next & TCP_STATE_MASK;

        /* 根据状态图进行状态转移 */
        tcp_set_state(sk, ns);

        /* 如果需要执行发送FIN的动作，则返回真 */
        return next & TCP_ACTION_FIN;
}
\end{minted}

		\subsubsection{\mintinline{c}{tcp_send_fin}}

\begin{minted}[linenos]{c}
/* 
Location:

	net/ipv4/tcp_output.c

Function:

	Send a FIN. The caller locks the socket for us.
	We should try to send a FIN packet really hard, but eventually give up.

Parameter:

	sk:传输控制块
*/
void tcp_send_fin(struct sock *sk)
{
		/*取得sock发送队列的最后一个元素，如果为空，返回null*/
        struct sk_buff *skb, *tskb = tcp_write_queue_tail(sk);
        struct tcp_sock *tp = tcp_sk(sk);

        /* 这里做了一些优化,如果发送队列的末尾还有段没有发出去，则利用该段发送FIN。 */
        if (tskb && (tcp_send_head(sk) || tcp_under_memory_pressure(sk))) {
          /* 如果当前正在发送的队列不为空，或者当前TCP处于内存压力下(值得进一步分析)，则进行该优化 */
coalesce:
                TCP_SKB_CB(tskb)->tcp_flags |= TCPHDR_FIN;
				//递增序列号
                TCP_SKB_CB(tskb)->end_seq++;
                tp->write_seq++;
				/*队头为空*/
                if (!tcp_send_head(sk)) {
                        /* This means tskb was already sent.
                         * Pretend we included the FIN on previous transmit.
                         * We need to set tp->snd_nxt to the value it would have
                         * if FIN had been sent. This is because retransmit path
                         * does not change tp->snd_nxt.
                         */
                        tp->snd_nxt++;
                        return;
                }
        } else {
                /* 为封包分配空间 */
                skb = alloc_skb_fclone(MAX_TCP_HEADER, sk->sk_allocation);
                if (unlikely(!skb)) {
                        /* 如果分配不到空间，且队尾还有未发送的包，利用该包发出FIN。 */
                        if (tskb)
                                goto coalesce;
                        return;
                }
                skb_reserve(skb, MAX_TCP_HEADER);
                sk_forced_mem_schedule(sk, skb->truesize);
                /* FIN eats a sequence byte, write_seq advanced by tcp_queue_skb(). */
                /* 构造一个FIN包，并加入发送队列。 */
                tcp_init_nondata_skb(skb, tp->write_seq,
                                     TCPHDR_ACK | TCPHDR_FIN);
                tcp_queue_skb(sk, skb);
        }
        __tcp_push_pending_frames(sk, tcp_current_mss(sk), TCP_NAGLE_OFF);
}
\end{minted}
	在函数的最后，将所有的剩余数据一口气发出去，完成发送FIN包的过程。
	至此，主动关闭过程的第一次握手完成。

	\subsection{第二次握手:接收ACK}
		在发出FIN后，接收端会回复ACK确认收到了请求。从这里开始有两种情况，一种是教科书式
		的四次握手的情况，另一种是双方同时发出FIN的情况，这里我们考虑前一种情况。
		后者会在\ref{subsec:fin_at_same_time}中描述。
		
		我们知道内核中接收数据都是从tcp\_v4\_do\_rcv函数开始，简单分析就知道会继续调用
		tcp\_rcv\_state\_process函数，我们就直接从这里开始了。
		\subsubsection{\mintinline{c}{tcp_rcv_state_process}}
			\label{ClientReceiveACK:tcp_rcv_state_process}
\begin{minted}[linenos]{c}
int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)
{
	struct tcp_sock *tp = tcp_sk(sk);
	struct inet_connection_sock *icsk = inet_csk(sk);
	const struct tcphdr *th = tcp_hdr(skb);
	struct request_sock *req;
	int queued = 0;
	bool acceptable;

	tp->rx_opt.saw_tstamp = 0;
	switch (sk->sk_state) {
		case TCP_CLOSE:
		        goto discard;

		case TCP_LISTEN:
		        /* LISTEN状态处理代码，略去 */

		case TCP_SYN_SENT:
		        /* SYN-SENT状态处理代码，略去 */
	}
	req = tp->fastopen_rsk;
	if (req) {
		WARN_ON_ONCE(sk->sk_state != TCP_SYN_RECV &&
		    sk->sk_state != TCP_FIN_WAIT1);

		if (!tcp_check_req(sk, skb, req, true))
			goto discard;
	}

	if (!th->ack && !th->rst && !th->syn)
		goto discard;

	if (!tcp_validate_incoming(sk, skb, th, 0))
		return 0;

	/* step 5: check the ACK field ，判断是否可以接收*/
	acceptable = tcp_ack(sk, skb, FLAG_SLOWPATH |
				      FLAG_UPDATE_TS_RECENT) > 0;
	switch (sk->sk_state) {
		case TCP_SYN_RECV:
		        /* SYN-RECV状态处理代码，略去 */
		case TCP_FIN_WAIT1: {
		        struct dst_entry *dst;
		        int tmo;

		        /* 如果当前的套接字为开启了Fast Open的套接字，且该ACK为
		         * 接收到的第一个ACK，那么这个ACK应该是在确认SYNACK包，
		         * 因此，停止SYNACK计时器。
		         */
		        if (req) {
		                /* Return RST if ack_seq is invalid.
		                 * Note that RFC793 only says to generate a
		                 * DUPACK for it but for TCP Fast Open it seems
		                 * better to treat this case like TCP_SYN_RECV
		                 * above.
		                 */
		                if (!acceptable)
		                        return 1;
		                /* 移除fastopen请求 */
		                reqsk_fastopen_remove(sk, req, false);
		                tcp_rearm_rto(sk);
		        }
				/*未确认的序列号是不是等于发送队列队尾序列号??*/
		        if (tp->snd_una != tp->write_seq)
		                break;

		        /* 收到ACK后，转移到TCP_FIN_WAIT2状态，将发送端关闭。 */
		        tcp_set_state(sk, TCP_FIN_WAIT2);
		        sk->sk_shutdown |= SEND_SHUTDOWN;

		        /* 如果路由缓存非空，确认路由缓存有效 */
		        dst = __sk_dst_get(sk);
		        if (dst)
		                dst_confirm(dst);

		        /* 唤醒等待该套接字的进程 */
		        if (!sock_flag(sk, SOCK_DEAD)) {
		                /* Wake up lingering close() */
		                sk->sk_state_change(sk);
		                break;
		        }

		        /* 如果linger2小于0,则说明无需在FIN_WAIT2状态等待，
					直接关闭传输控制块。 */
		        if (tp->linger2 < 0 ||
		            (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&
		             after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt))) {
		                tcp_done(sk);
		                NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPABORTONDATA);
		                return 1;
		        }
\end{minted}

	转换到\mintinline{c}{TCP_FIN_WAIT2}以后，计算接受fin包的超时时间。
	如果还能留出TIMEWAIT阶段的时间（TIMEWAIT阶段有最长时间限制），那么在此之前，
	就激活保活计时器保持连接。如果时间已经不足了，就主动调用\mintinline{c}{tcp_time_wait}
	进入TIMEWAIT状态。
\begin{minted}[linenos]{c}
                tmo = tcp_fin_time(sk);
                if (tmo > TCP_TIMEWAIT_LEN) {
                        inet_csk_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);
                } else if (th->fin || sock_owned_by_user(sk)) {
                        /* Bad case. We could lose such FIN otherwise.
                         * It is not a big problem, but it looks confusing
                         * and not so rare event. We still can lose it now,
                         * if it spins in bh_lock_sock(), but it is really
                         * marginal case.
                         */
                        inet_csk_reset_keepalive_timer(sk, tmo);
                } else {
                        /* 进入TCP_FIN_WAIT2状态等待。 */
                        tcp_time_wait(sk, TCP_FIN_WAIT2, tmo);
                        goto discard;
                }
                break;

                /* 其余状态处理代码，略去 */
        }

        /* step 6: check the URG bit */
        tcp_urg(sk, skb, th);

        /* step 7: process the segment text */
        switch (sk->sk_state) {
          /* 其他状态处理代码，略去 */

		    case TCP_FIN_WAIT1:
		    case TCP_FIN_WAIT2:
		            /* RFC 793 says to queue data in these states,
		             * RFC 1122 says we MUST send a reset.
		             * BSD 4.4 also does reset.
		             */
					/*
						如果接收方向已经关闭，而又收到新的数据，则需要给对方发送
						RST段，告诉对方已经把数据丢弃。
					*/
		            if (sk->sk_shutdown & RCV_SHUTDOWN) {
		                    if (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&
		                        after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {
		                            NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPABORTONDATA);
		                            /* 如果接收端已经关闭了，那么发送RESET。 */
		                            tcp_reset(sk);
		                            return 1;
		                    }
		            }
		            /* Fall through */

		       /* 其他状态处理代码，略去 */
        }

        /* tcp_data could move socket to TIME-WAIT 
			如果TCP的状态不处于CLOSE状态，则发送队列中的段，
			同时调度ACK，确定是立即发送，还是延时发送。
		*/
        if (sk->sk_state != TCP_CLOSE) {
                tcp_data_snd_check(sk);
                tcp_ack_snd_check(sk);
        }

        if (!queued) {
discard:
                __kfree_skb(skb);
        }
        return 0;
}
\end{minted}
	执行完该段代码后，则进入了\mintinline{text}{FIN_WAIT2}状态。

	由于进入到\mintinline{text}{FIN_WAIT2}状态后，不会再处理TCP段的数据。
	因此，出于资源和方面的考虑，Linux内核采用了一个较小的结构体\mintinline{c}{tcp_timewait_sock}来
	取代正常的TCP传输控制块。\mintinline{text}{TIME_WAIT}也是可作同样处理。
	该替换过程通过函数\mintinline{c}{tcp_time_wait}完成。

		\subsubsection{\mintinline{c}{tcp_time_wait}}
\begin{minted}[linenos]{c}
/*
Location:

	net/ipv4/tcp_minicock.c

Function:

	Move a socket to time-wait or dead fin-wait-2 state.

Parameter:

	sk: sock
	state:状态
	timeo:超时时间
	
*/
void tcp_time_wait(struct sock *sk, int state, int timeo)
{
        const struct inet_connection_sock *icsk = inet_csk(sk);
        const struct tcp_sock *tp = tcp_sk(sk);
        struct inet_timewait_sock *tw;
        bool recycle_ok = false;
		/*	
			如果启用tw_recycle，且ts_recent_stamp有效，则记录相关时间戳
			信息到对端信息管理块中。
		*/
        if (tcp_death_row.sysctl_tw_recycle && tp->rx_opt.ts_recent_stamp)
                recycle_ok = tcp_remember_stamp(sk);

        /* 分配空间 */
        tw = inet_twsk_alloc(sk, &tcp_death_row, state);
		/*分配成功*/
        if (tw) {
                struct tcp_timewait_sock *tcptw = tcp_twsk((struct sock *)tw);
				/*计算重传超时时间*/
                const int rto = (icsk->icsk_rto << 2) - (icsk->icsk_rto >> 1);
                struct inet_sock *inet = inet_sk(sk);

                /* 将值复制给对应的域 */
                tw->tw_transparent      = inet->transparent;
                tw->tw_rcv_wscale       = tp->rx_opt.rcv_wscale;
                tcptw->tw_rcv_nxt       = tp->rcv_nxt;
                tcptw->tw_snd_nxt       = tp->snd_nxt;
                tcptw->tw_rcv_wnd       = tcp_receive_window(tp);
                tcptw->tw_ts_recent     = tp->rx_opt.ts_recent;
                tcptw->tw_ts_recent_stamp = tp->rx_opt.ts_recent_stamp;
                tcptw->tw_ts_offset     = tp->tsoffset;
                tcptw->tw_last_oow_ack_time = 0;

                /* 部分对于ipv6和md5的处理，略过 */

                /* Get the TIME_WAIT timeout firing. */
                if (timeo < rto)
                        timeo = rto;
				/*
					如果成功将相关时间戳信息添加到对端信息管理块中，
					则TIME_WAIT的超时时间设置为3.5倍的往返时间。
					否则，设置为60s。
				*/
                if (recycle_ok) {
                        tw->tw_timeout = rto;
                } else {
                        tw->tw_timeout = TCP_TIMEWAIT_LEN;
                        if (state == TCP_TIME_WAIT)
                                timeo = TCP_TIMEWAIT_LEN;
                }

                /* 启动定时器 */
                inet_twsk_schedule(tw, timeo);
                /* 将timewait控制块插入到哈希表中，替代原有的传输控制块 */
                __inet_twsk_hashdance(tw, sk, &tcp_hashinfo);
                inet_twsk_put(tw);
        } else {
                /* 当内存不够时，直接关闭连接 */
                NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPTIMEWAITOVERFLOW);
        }

        /* 更新一些测量值并关闭原来的传输控制块 */
        tcp_update_metrics(sk);
        tcp_done(sk);
}
\end{minted}

	\subsection{第三次握手:接受FIN}
		\label{subsec:third_recv_fin}
			此时，由于已经使用了timewait控制块取代了TCP控制块。因此，对应的处理代码不再位于
			\mintinline{c}{tcp_rcv_state_process}中，而是换到了
			\mintinline{c}{tcp_timewait_state_process}函数中。
		\subsubsection{\mintinline{c}{tcp_timewait_state_process}}
			\label{ClientReceiveFIN:tcp_timewait_state_process}
			该函数的代码如下，可以看到
			参数中已经变成了\mintinline{c}{inet_timewait_sock}。

\begin{minted}[linenos]{c}
/*
Location

	net/ipv4/tcp_minisock.h

Description

 * * Main purpose of TIME-WAIT state is to close connection gracefully,
 *   when one of ends sits in LAST-ACK or CLOSING retransmitting FIN
 *   (and, probably, tail of data) and one or more our ACKs are lost.
 * * What is TIME-WAIT timeout? It is associated with maximal packet
 *   lifetime in the internet, which results in wrong conclusion, that
 *   it is set to catch "old duplicate segments" wandering out of their path.
 *   It is not quite correct. This timeout is calculated so that it exceeds
 *   maximal retransmission timeout enough to allow to lose one (or more)
 *   segments sent by peer and our ACKs. This time may be calculated from RTO.
 * * When TIME-WAIT socket receives RST, it means that another end
 *   finally closed and we are allowed to kill TIME-WAIT too.
 * * Second purpose of TIME-WAIT is catching old duplicate segments.
 *   Well, certainly it is pure paranoia, but if we load TIME-WAIT
 *   with this semantics, we MUST NOT kill TIME-WAIT state with RSTs.
 * * If we invented some more clever way to catch duplicates
 *   (f.e. based on PAWS), we could truncate TIME-WAIT to several RTOs.
 *
 * The algorithm below is based on FORMAL INTERPRETATION of RFCs.
 * When you compare it to RFCs, please, read section SEGMENT ARRIVES
 * from the very beginning.
 *
 * NOTE. With recycling (and later with fin-wait-2) TW bucket
 * is _not_ stateless. It means, that strictly speaking we must
 * spinlock it. I do not want! Well, probability of misbehaviour
 * is ridiculously low and, seems, we could use some mb() tricks
 * to avoid misread sequence numbers, states etc.  --ANK
 *
 * We don't need to initialize tmp_out.sack_ok as we don't use the results

Parameter

	tw:接收处理段的timewait控制块。
	skb:FIN_WAIT2和TIME_WAIT状态下接收到的段
	th:TCP的首部
*/
enum tcp_tw_status
tcp_timewait_state_process(struct inet_timewait_sock *tw, struct sk_buff *skb,
                           const struct tcphdr *th)
{
        struct tcp_options_received tmp_opt;
        struct tcp_timewait_sock *tcptw = tcp_twsk((struct sock *)tw);
        bool paws_reject = false;
		/*首先标记没有看到时间戳*/
        tmp_opt.saw_tstamp = 0;
        if (th->doff > (sizeof(*th) >> 2) && tcptw->tw_ts_recent_stamp) {
                tcp_parse_options(skb, &tmp_opt, 0, NULL);

                if (tmp_opt.saw_tstamp) {
                        tmp_opt.rcv_tsecr       -= tcptw->tw_ts_offset;
                        tmp_opt.ts_recent       = tcptw->tw_ts_recent;
                        tmp_opt.ts_recent_stamp = tcptw->tw_ts_recent_stamp;
                        paws_reject = tcp_paws_reject(&tmp_opt, th->rst);
                }
        }
\end{minted}
	检测收到的包是否含有时间戳选项，如果有，则进行PAWS相关的检测。之后，开始进行
	\mintinline{text}{TCP_FIN_WAIT2}相关的处理。

	关于tcp\_paws\_reject更多的内容，请参见\ref{TCPPAWS:tcp_paws_reject}.
\begin{minted}[linenos]{c}
        if (tw->tw_substate == TCP_FIN_WAIT2) {
                /* 重复tcp_rcv_state_process()所进行的所有检测 */

                /* 序号不在窗口内或序号无效，发送ACK */
                if (paws_reject ||
                    !tcp_in_window(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq,
                                   tcptw->tw_rcv_nxt,
                                   tcptw->tw_rcv_nxt + tcptw->tw_rcv_wnd))
                        return tcp_timewait_check_oow_rate_limit(
                                tw, skb, LINUX_MIB_TCPACKSKIPPEDFINWAIT2);

                /* 如果收到RST包，则销毁timewait控制块并返回TCP_TW_SUCCESS */
                if (th->rst)
                        goto kill;

                /* 如果收到过期的SYN包，则销毁并发送RST */
                if (th->syn && !before(TCP_SKB_CB(skb)->seq, tcptw->tw_rcv_nxt))
                        goto kill_with_rst;

                /* 如果收到DACK，则释放该控制块，返回TCP_TW_SUCCESS*/
                if (!th->ack ||
                    !after(TCP_SKB_CB(skb)->end_seq, tcptw->tw_rcv_nxt) ||
                    TCP_SKB_CB(skb)->end_seq == TCP_SKB_CB(skb)->seq) {
                        inet_twsk_put(tw);
                        return TCP_TW_SUCCESS;
                }

                /* 之后只有两种情况，有新数据或收到FIN包 */
                if (!th->fin ||
                    TCP_SKB_CB(skb)->end_seq != tcptw->tw_rcv_nxt + 1) {
                        /* 如果收到了新的数据或者序号有问题，
                         * 则销毁控制块并返回TCP_TW_RST。 
                         */
kill_with_rst:
                        inet_twsk_deschedule_put(tw);
                        return TCP_TW_RST;
                }

                /* 收到了FIN包，进入TIME_WAIT状态 */
                tw->tw_substate   = TCP_TIME_WAIT;
                tcptw->tw_rcv_nxt = TCP_SKB_CB(skb)->end_seq;
                /* 如果启用了时间戳选项，则设置相关属性 */
                if (tmp_opt.saw_tstamp) {
                        tcptw->tw_ts_recent_stamp = get_seconds();
                        tcptw->tw_ts_recent       = tmp_opt.rcv_tsval;
                }

                /* 启动TIME_WAIT定时器 */
                if (tcp_death_row.sysctl_tw_recycle &&
                    tcptw->tw_ts_recent_stamp &&
                    tcp_tw_remember_stamp(tw))
                        inet_twsk_reschedule(tw, tw->tw_timeout);
                else
                        inet_twsk_reschedule(tw, TCP_TIMEWAIT_LEN);
                return TCP_TW_ACK;
        }

        /* TIME_WAIT阶段处理代码 */

}
\end{minted}

	\subsection{第四次握手:发送ACK}
		在\mintinline{c}{tcp_v4_rcv}中，如果发现目前的连接处于\mintinline{text}{FIN_WAIT2}
		或\mintinline{text}{TIME_WAIT}状态，则调用\mintinline{c}{tcp_timewait_state_process}
		进行处理，根据其返回值，执行相关操作。

\begin{minted}[linenos]{c}
switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {
        case TCP_TW_SYN: {
                struct sock *sk2 = inet_lookup_listener(dev_net(skb->dev),
                                                        &tcp_hashinfo,
                                                        iph->saddr, th->source,
                                                        iph->daddr, th->dest,
                                                        inet_iif(skb));
                if (sk2) {
                        inet_twsk_deschedule_put(inet_twsk(sk));
                        sk = sk2;
                        goto process;
                }
                /* Fall through to ACK */
        }
        case TCP_TW_ACK:
                /* 回复ACK包 */
                tcp_v4_timewait_ack(sk, skb);
                break;
        case TCP_TW_RST:
                goto no_tcp_socket;
        case TCP_TW_SUCCESS:;
}
\end{minted}
		根据上面的分析，在正常情况下，\mintinline{c}{tcp_timewait_state_process}会返回
		\mintinline{c}{TCP_TW_ACK}，因此，会调用\mintinline{c}{tcp_v4_timewait_ack}。
		该函数如下：
\begin{minted}[linenos]{c}
static void tcp_v4_timewait_ack(struct sock *sk, struct sk_buff *skb)
{
        struct inet_timewait_sock *tw = inet_twsk(sk);
        struct tcp_timewait_sock *tcptw = tcp_twsk(sk);

        /* 发送ACK包 */
        tcp_v4_send_ack(sock_net(sk), skb,
                        tcptw->tw_snd_nxt, tcptw->tw_rcv_nxt,
                        tcptw->tw_rcv_wnd >> tw->tw_rcv_wscale,
                        tcp_time_stamp + tcptw->tw_ts_offset,
                        tcptw->tw_ts_recent,
                        tw->tw_bound_dev_if,
                        tcp_twsk_md5_key(tcptw),
                        tw->tw_transparent ? IP_REPLY_ARG_NOSRCCHECK : 0,
                        tw->tw_tos
                        );

        /* 释放timewait控制块 */
        inet_twsk_put(tw);
}
\end{minted}
	紧接着又将发送ACK包的任务交给\mintinline{c}{tcp_v4_send_ack}来执行。
\begin{minted}[linenos]{c}
/* 下面的代码负责在SYN_RECV和TIME_WAIT状态下发送ACK包。
 *
 * The code following below sending ACKs in SYN-RECV and TIME-WAIT states
 * outside socket context is ugly, certainly. What can I do? 
 */
static void tcp_v4_send_ack(struct net *net,
                            struct sk_buff *skb, u32 seq, u32 ack,
                            u32 win, u32 tsval, u32 tsecr, int oif,
                            struct tcp_md5sig_key *key,
                            int reply_flags, u8 tos)
{
        const struct tcphdr *th = tcp_hdr(skb);
        struct {
                struct tcphdr th;
                __be32 opt[(TCPOLEN_TSTAMP_ALIGNED >> 2)
#ifdef CONFIG_TCP_MD5SIG
                           + (TCPOLEN_MD5SIG_ALIGNED >> 2)
#endif
                        ];
        } rep;
        struct ip_reply_arg arg;

        memset(&rep.th, 0, sizeof(struct tcphdr));
        memset(&arg, 0, sizeof(arg));

        /* 构造参数和TCP头部 */
        arg.iov[0].iov_base = (unsigned char *)&rep;
        arg.iov[0].iov_len  = sizeof(rep.th);
        if (tsecr) {
                rep.opt[0] = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |
                                   (TCPOPT_TIMESTAMP << 8) |
                                   TCPOLEN_TIMESTAMP);
                rep.opt[1] = htonl(tsval);
                rep.opt[2] = htonl(tsecr);
                arg.iov[0].iov_len += TCPOLEN_TSTAMP_ALIGNED;
        }

        /* 交换发送端和接收端 */
        rep.th.dest    = th->source;
        rep.th.source  = th->dest;
        rep.th.doff    = arg.iov[0].iov_len / 4;
        rep.th.seq     = htonl(seq);
        rep.th.ack_seq = htonl(ack);
        rep.th.ack     = 1;
        rep.th.window  = htons(win);

        /* 略去和MD5相关的部分代码 */

        /* 设定标志位和校验码 */
        arg.flags = reply_flags;
        arg.csum = csum_tcpudp_nofold(ip_hdr(skb)->daddr,
                                      ip_hdr(skb)->saddr, /* XXX */
                                      arg.iov[0].iov_len, IPPROTO_TCP, 0);
        arg.csumoffset = offsetof(struct tcphdr, check) / 2;
        if (oif)
                arg.bound_dev_if = oif;
        arg.tos = tos;
        /* 调用IP层接口将包发出 */
        ip_send_unicast_reply(*this_cpu_ptr(net->ipv4.tcp_sk),
                              skb, &TCP_SKB_CB(skb)->header.h4.opt,
                              ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,
                              &arg, arg.iov[0].iov_len);

        TCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);
}
\end{minted}
至此，四次握手就完成了。

	\subsection{同时关闭}
		\label{subsec:fin_at_same_time}
			还有一种情况是双方同时发出了FIN报文，准备关闭连接。表现在TCP的状态图上，就是在
			发出FIN包以后又收到了FIN包，因此进入了CLOSING状态。该段代码在\ref{subsubsec:tcp_fin}
			中进行了解析。之后，CLOSING状态等待接受ACK，就会进入到下一个状态
			在\mintinline{c}{tcp_rcv_state_process}中，处理\mintinline{text}{TCP_CLOSING}的代码如下：
\begin{minted}[linenos]{c}
        case TCP_CLOSING:
                if (tp->snd_una == tp->write_seq) {
                        tcp_time_wait(sk, TCP_TIME_WAIT, 0);
                        goto discard;
                }
                break;
\end{minted}
	如果收到的ACK没问题则转入\mintinline{text}{TIME_WAIT}状态，利用timewait
	控制块完成后续的工作。
\begin{minted}[linenos]{c}
switch (sk->sk_state) {
        case TCP_CLOSE_WAIT:
        case TCP_CLOSING:
        case TCP_LAST_ACK:
                if (!before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt))
                        break;
        case TCP_FIN_WAIT1:
        case TCP_FIN_WAIT2:
                /* RFC 793 says to queue data in these states,
                 * RFC 1122 says we MUST send a reset.
                 * BSD 4.4 also does reset.
                 */
                if (sk->sk_shutdown & RCV_SHUTDOWN) {
                        if (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&
                            after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {
                                NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPABORTONDATA);
                                tcp_reset(sk);
                                return 1;
                        }
                }
                /* Fall through */
        case TCP_ESTABLISHED:
                tcp_data_queue(sk, skb);
                queued = 1;
                break;
}
\end{minted}
如果段中的数据正常，且接口没有关闭，那么就收下数据。否则，就直接忽略掉数据段的数据。

	\subsection{TIME\_WAIT}
		\label{subsec:time_wait}
			该状态的处理也在\mintinline{c}{tcp_timewait_state_process}函数中。紧接在
			\ref{subsec:third_recv_fin}中处理\mintinline{text}{FIN_WAIT2}状态的代码之后。
			此时，说明当前的状态为\mintinline{text}{TIME_WAIT}。
\begin{minted}[linenos]{c}
        /*
         *      Now real TIME-WAIT state.
         *
         *      RFC 1122:
         *      "When a connection is [...] on TIME-WAIT state [...]
         *      [a TCP] MAY accept a new SYN from the remote TCP to
         *      reopen the connection directly, if it:
         *
         *      (1)  assigns its initial sequence number for the new
         *      connection to be larger than the largest sequence
         *      number it used on the previous connection incarnation,
         *      and
         *
         *      (2)  returns to TIME-WAIT state if the SYN turns out
         *      to be an old duplicate".
         */
        /* 
			序号没有回卷,且正是预期接收的段，段中没有负载
			或段中存在RST标志。
		*/
        if (!paws_reject &&
            (TCP_SKB_CB(skb)->seq == tcptw->tw_rcv_nxt &&
             (TCP_SKB_CB(skb)->seq == TCP_SKB_CB(skb)->end_seq || th->rst))) {
				/*如果存在RST*/
                if (th->rst) {
                        /* This is TIME_WAIT assassination, in two flavors.
                         * Oh well... nobody has a sufficient solution to this
                         * protocol bug yet.
                         */
                        if (sysctl_tcp_rfc1337 == 0) {
kill:
                                inet_twsk_deschedule_put(tw);
                                return TCP_TW_SUCCESS;
                        }
                }
                /* 重新激活定时器 */
                inet_twsk_reschedule(tw, TCP_TIMEWAIT_LEN);

                if (tmp_opt.saw_tstamp) {
                        tcptw->tw_ts_recent       = tmp_opt.rcv_tsval;
                        tcptw->tw_ts_recent_stamp = get_seconds();
                }

                inet_twsk_put(tw);
                return TCP_TW_SUCCESS;
        }
\end{minted}
如果处于\mintinline{c}{TIME_WAIT}状态时，受到了Reset包，那么，按照TCP协议的要求，
应当重置连接。但这里就产生了一个问题。本来\mintinline{c}{TIME_WAIT}之所以要等待2MSL
的时间，就是为了避免在网络上滞留的包对新的连接造成影响。但是，此处却可以通过发送rst报文
强行重置连接。重置意味着该连接会被强行关闭，跳过了2MSL阶段。这样就和设立2MSL的初衷不符了。
具体的讨论见\ref{subsec:rfc1337}。如果启用了RFC1337，那么就会忽略掉这个RST报文。

\begin{minted}[linenos]{c}
        /* 之后是超出窗口范围的情况。

           All the segments are ACKed immediately.

           The only exception is new SYN. We accept it, if it is
           not old duplicate and we are not in danger to be killed
           by delayed old duplicates. RFC check is that it has
           newer sequence number works at rates <40Mbit/sec.
           However, if paws works, it is reliable AND even more,
           newer sequence number works at rates <40Mbit/sec.
           However, if paws works, it is reliable AND even more,
           we even may relax silly seq space cutoff.

           RED-PEN: we violate main RFC requirement, if this SYN will appear
           old duplicate (i.e. we receive RST in reply to SYN-ACK),
           we must return socket to time-wait state. It is not good,
           but not fatal yet.
         */
		/*
			接收到SYN段，并且没有RST和ACK标志，并且没有拒绝检测，并且序号有效。
		*/
        if (th->syn && !th->rst && !th->ack && !paws_reject &&
            (after(TCP_SKB_CB(skb)->seq, tcptw->tw_rcv_nxt) ||
             (tmp_opt.saw_tstamp &&
              (s32)(tcptw->tw_ts_recent - tmp_opt.rcv_tsval) < 0))) {
                /* 如果可以接受该SYN请求，那么重新计算isn号，并返回TCP_TW_SYN。 */
                u32 isn = tcptw->tw_snd_nxt + 65535 + 2;
                if (isn == 0)
                        isn++;
                TCP_SKB_CB(skb)->tcp_tw_isn = isn;
                return TCP_TW_SYN;
        }

        if (paws_reject)
                NET_INC_STATS_BH(twsk_net(tw), LINUX_MIB_PAWSESTABREJECTED);
\end{minted}
此后，如果收到了序号绕回的包，那么就重置\mintinline{c}{TIME_WAIT}定时器，并返回
\mintinline{c}{TCP_TW_ACK}。
\begin{minted}[linenos]{c}
        if (!th->rst) {
                /* In this case we must reset the TIMEWAIT timer.
                 *
                 * If it is ACKless SYN it may be both old duplicate
                 * and new good SYN with random sequence number <rcv_nxt.
                 * Do not reschedule in the last case.
                 */
                if (paws_reject || th->ack)
                        inet_twsk_reschedule(tw, TCP_TIMEWAIT_LEN);

                return tcp_timewait_check_oow_rate_limit(
                        tw, skb, LINUX_MIB_TCPACKSKIPPEDTIMEWAIT);
        }
        inet_twsk_put(tw);
        return TCP_TW_SUCCESS;
}
\end{minted}

